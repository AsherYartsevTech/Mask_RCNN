{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /Users/AsherYartsev/Mask_RCNN/logs/shapes20181127T2334/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py:46: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 1490s 15s/step - loss: 2.0722 - rpn_class_loss: 0.0314 - rpn_bbox_loss: 0.6397 - mrcnn_class_loss: 0.3740 - mrcnn_bbox_loss: 0.4004 - mrcnn_mask_loss: 0.6266 - val_loss: 1.0885 - val_rpn_class_loss: 0.0151 - val_rpn_bbox_loss: 0.4193 - val_mrcnn_class_loss: 0.0941 - val_mrcnn_bbox_loss: 0.1885 - val_mrcnn_mask_loss: 0.3714\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /Users/AsherYartsev/Mask_RCNN/logs/shapes20181127T2334/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py:46: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "100/100 [==============================] - 1717s 17s/step - loss: 1.0299 - rpn_class_loss: 0.0172 - rpn_bbox_loss: 0.4067 - mrcnn_class_loss: 0.1092 - mrcnn_bbox_loss: 0.1451 - mrcnn_mask_loss: 0.3516 - val_loss: 0.8356 - val_rpn_class_loss: 0.0157 - val_rpn_bbox_loss: 0.3525 - val_mrcnn_class_loss: 0.0863 - val_mrcnn_bbox_loss: 0.1042 - val_mrcnn_mask_loss: 0.2768\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "Loading weights from  /Users/AsherYartsev/Mask_RCNN/logs/shapes20181127T2334/mask_rcnn_shapes_0002.h5\n",
      "Re-starting from epoch 2\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:   28.00000  max:  237.00000  uint8\n",
      "image_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int64\n",
      "gt_class_id              shape: (1,)                  min:    1.00000  max:    1.00000  int32\n",
      "gt_bbox                  shape: (1, 4)                min:   29.00000  max:   84.00000  int32\n",
      "gt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:   28.00000  max:  237.00000  uint8\n",
      "molded_images            shape: (1, 128, 128, 3)      min:  -84.70000  max:  120.20000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max:  128.00000  int64\n",
      "anchors                  shape: (1, 4092, 4)          min:   -0.71267  max:    1.20874  float32\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.818333334326744\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
